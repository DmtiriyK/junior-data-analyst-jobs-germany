{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22207595-2374-483d-a8cb-773fd03ee685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Setup headers for HTTP requests\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "jobs_list = []\n",
    "\n",
    "# Loop through 10 pages (each page has 25 job results)\n",
    "for start in range(0, 250, 25):\n",
    "    url = f'https://www.linkedin.com/jobs/search/?keywords=Junior%20Data%20Analyst&location=Germany&start={start}'\n",
    "    print(f'ðŸ” Scraping: {url}')\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    job_cards = soup.find_all('div', class_='base-card')\n",
    "\n",
    "    print(f'   âž¤ Found: {len(job_cards)} job listings')\n",
    "\n",
    "    for job in job_cards:\n",
    "        # Extract job title, company, location, and job link\n",
    "        try:\n",
    "            title = job.find('h3', class_='base-search-card__title').text.strip()\n",
    "        except:\n",
    "            title = ''\n",
    "        try:\n",
    "            company = job.find('h4', class_='base-search-card__subtitle').text.strip()\n",
    "        except:\n",
    "            company = ''\n",
    "        try:\n",
    "            location = job.find('span', class_='job-search-card__location').text.strip()\n",
    "        except:\n",
    "            location = ''\n",
    "        try:\n",
    "            link = job.find('a', class_='base-card__full-link')['href']\n",
    "        except:\n",
    "            link = ''\n",
    "\n",
    "        jobs_list.append({\n",
    "            'Title': title,\n",
    "            'Company': company,\n",
    "            'Location': location,\n",
    "            'Link': link\n",
    "        })\n",
    "\n",
    "    time.sleep(2)  # Pause between pages to avoid getting blocked\n",
    "\n",
    "# Collect job descriptions\n",
    "print(\"ðŸ“ Fetching job descriptions...\")\n",
    "descriptions = []\n",
    "\n",
    "for i, job in enumerate(jobs_list):\n",
    "    url = job['Link']\n",
    "    print(f\"{i+1}/{len(jobs_list)} | {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        desc_block = soup.find('div', {'class': lambda x: x and 'description' in x})\n",
    "        if desc_block:\n",
    "            description = desc_block.get_text(separator=' ', strip=True)\n",
    "        else:\n",
    "            description = 'Not found'\n",
    "    except Exception as e:\n",
    "        print(\"âŒ Error:\", e)\n",
    "        description = 'Error'\n",
    "    descriptions.append(description)\n",
    "    time.sleep(1.5)  # Be respectful with delays\n",
    "\n",
    "# Create a DataFrame and attach descriptions\n",
    "df = pd.DataFrame(jobs_list)\n",
    "df['Description'] = descriptions\n",
    "\n",
    "# Normalize text for analysis\n",
    "df['description_lower'] = df['Description'].fillna('').str.lower()\n",
    "\n",
    "# Flag if remote/hybrid work is mentioned\n",
    "df['Remote'] = df['description_lower'].apply(\n",
    "    lambda x: any(word in x for word in ['remote', 'home office', 'hybrid'])\n",
    ")\n",
    "\n",
    "# Detect presence of specific skills in job descriptions\n",
    "skills_flags = ['python', 'sql', 'excel', 'power bi', 'tableau', 'r', 'git', 'vba', 'sas']\n",
    "for skill in skills_flags:\n",
    "    col_name = f'Has_{skill.replace(\" \", \"_\").upper()}'\n",
    "    df[col_name] = df['description_lower'].apply(lambda x: skill in x)\n",
    "\n",
    "# Basic language detection based on keywords\n",
    "def detect_language(text):\n",
    "    text = text.lower()\n",
    "    en_count = sum(word in text for word in ['responsibilities', 'requirements', 'apply', 'english'])\n",
    "    de_count = sum(word in text for word in ['anforderungen', 'deutsch', 'bewerben', 'verantwortung'])\n",
    "    if en_count > de_count and en_count >= 2:\n",
    "        return 'en'\n",
    "    elif de_count > en_count and de_count >= 2:\n",
    "        return 'de'\n",
    "    else:\n",
    "        return 'mixed'\n",
    "\n",
    "df['Lang'] = df['description_lower'].apply(detect_language)\n",
    "\n",
    "# Drop intermediate column\n",
    "df.drop(columns=['description_lower'], inplace=True)\n",
    "\n",
    "# Save final dataset to CSV\n",
    "df.to_csv(\"linkedin_jobs_enriched.csv\", index=False)\n",
    "print(\"âœ… Done! Total jobs saved:\", len(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
